##### В чем особенность SVM по сравнению с другими линейными классификаторами?
- [x] Использование кусочно-линейной аппроксимации функций потерь и квадратичного регуляризатора
- [ ] Это два разных метода, не имеющих ничего общего между собой
- [ ] Добавление регуляризатора
- [ ] Введение кусочно-линейной аппроксимации функции потерь

##### Зачем в SVM вводятся ядра?
- [x] Чтобы лучше описывать более сложные закономерности в данных
- [ ] Потому что линейный SVM умеет работать только с линейно разделимыми данными
- [ ] Для упрощения задачи оптимизации
- [x] Без ядер нельзя построить нелинейную разделяющую поверхность

##### 1. Какое из этих утверждений относится к методу опорных векторов
- [ ] Его предсказание зависит лишь от части признаков
- [ ] Он строит разделяющую гиперплоскость, максимально отдаленную от объектов обучающей выборки
- [x] Его предсказание зависит лишь от части объектов

##### 2. Почему для метода опорных векторов возможно нелинейное обобщение
- [ ] Задачу метода опорных векторов можно переписать так, что она будет зависеть только от скалярных произведений объектов.
      Эти скалярные произведения можно заменить на евклидовы расстояния между объектами, благодаря чему итоговоый алгоритм 
      будет нелинейным
- [x] Задачу метода опорных векторов можно переписать так, что она будет зависеть только от скалярных произведений объектов.
      Эти скалярные произведения можно заменить на функцию, вычисляющую скалярное произведение в пространстве более высокой размерности
- [ ] Исходные признаки можно расширить, добавив результаты применения к ним различных нелинейных функций. Классификатор, обученный
      методом опорных векторов по таким признакам, будет являться нелинейным относительно исходных признаков

####  Логистическая регрессия
##### 1. Выберите верные утверждения про логистическую регрессию
- [x] Вероятность принадлежности объекта к положительному классу оценивается как сигмоида от скалярного произведения весов на признаки
- [x] Настройка логистической регрессии эквивалентна максимизации правдоподобия ответов классификатора
- [x] Логическая регрессия оценивает вероятности принадлежности объектов к классам

##### 2. Выберите верные утверждения про логистическую регрессию
- [x] Регуляризация позволяет бороться с переобучением
- [ ] Регуляризация позволяет оценивать вероятности классов с помощью алгоритма
- [x] L1 - регуляризация позволяет отбирать признаки
- [ ] L1 - регуляризация представляет собой сумму квадратов весов


####  Метрики качества классификации
##### 2. Выберите верные утверждения про площадь под ROC-кривой (AUC-ROC)
- [ ] ROC-кривая строится в осях "доля верных отрицательных классификаций" и "доля верных положительных классификаций"
- [x] ROC-кривая строится в осях "доля ошибочных положительных классификаций" и "доля верных положительных классификаций"
- [ ] Площадь под ROC-кривой для идеального алгоритма зависит от доли положительных объектов в выборке  
- [x] Площадь под ROC-кривой для идеального алгоритма равна 1












