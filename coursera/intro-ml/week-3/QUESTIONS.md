##### В чем особенность SVM по сравнению с другими линейными классификаторами?
- [x] Использование кусочно-линейной аппроксимации функций потерь и квадратичного регуляризатора
- [ ] Это два разных метода, не имеющих ничего общего между собой
- [ ] Добавление регуляризатора
- [ ] Введение кусочно-линейной аппроксимации функции потерь

##### Зачем в SVM вводятся ядра?
- [x] Чтобы лучше описывать более сложные закономерности в данных
- [ ] Потому что линейный SVM умеет работать только с линейно разделимыми данными
- [ ] Для упрощения задачи оптимизации
- [x] Без ядер нельзя построить нелинейную разделяющую поверхность


##### 1. Какое из этих утверждений относится к методу опорных векторов
- [ ] Его предсказание зависит лишь от части признаков
- [ ] Он строит разделяющую гиперплоскость, максимально отдаленную от объектов обучающей выборки
- [x] Его предсказание зависит лишь от части объектов

##### 2. Почему для метода опорных векторов возможно нелинейное обобщение
- [ ] Задачу метода опорных векторов можно переписать так, что она будет зависеть только от скалярных произведений объектов.
      Эти скалярные произведения можно заменить на евклидовы расстояния между объектами, благодаря чему итоговоый алгоритм 
      будет нелинейным
- [x] Задачу метода опорных векторов можно переписать так, что она будет зависеть только от скалярных произведений объектов.
      Эти скалярные произведения можно заменить на функцию, вычисляющую скалярное произведение в пространстве более высокой размерности
- [ ] Исходные признаки можно расширить, добавив результаты применения к ним различных нелинейных функций. Классификатор, обученный
      методом опорных векторов по таким признакам, будет являться нелинейным относительно исходных признаков



