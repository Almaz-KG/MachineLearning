#### НЕДЕЛЯ 3 
#### Метод опорных векторов

`Метод опорных векторов (англ. SVM, support vector machine)` — набор схожих алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. 
Особым свойством метода опорных векторов является непрерывное уменьшение эмпирической ошибки классификации и увеличение зазора, поэтому метод также известен как метод классификатора с максимальным зазором.

Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. 
Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. 

Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.
Данный метод решает задачи классификации и регрессии путем построения нелинейной плоскости, разделяющей решения.

Разделяющая гиперплоскость – это гиперплоскость, которая отделяет группу объектов, имеющих различную классовую принадлежность.

Для построения оптимальной гиперплоскости, SVM прибегает к итерационному алгоритму обучения, использующемуся для минимизации функции ошибок.
В зависимости от вида функции ошибки, SVM модели можно разделить на четыре группы:
1. Классификация SVM типа 1 (также известна как C-SVM классификация)
2. Классификация SVM типа 2 (также известна как Nu-SVM классификация)
3. Регрессия SVM типа 1 (также известна как эпсилон-SVM регрессия)
4. Регрессия SVM типа 2 (также известна как Nu-SVM регрессия)

Ядра для нелинейного обощения SVM
Аппроксимация и регуляризация эмпирического риска
Оптимальная разделяющая гиперплоскость
Переход к линейно неразделимой выборке
Условия Каруша-Куна-Таккера
Ширина разделяющей поверхности

Преимущества метода опорных векторов (SVM)
- Задача выпуклого квадратичного программирования имеет единственное решение
- Выделяется множество опорных векторов
- Имеются эффективгые численные методы для SVM
- Изящное обобщение на нелинейные классификаторы
Недостати метода опорных векторов (SVM)
- Опорными объектами могут становиться объекты выбросы
- Нет отбора признаков в исходном пространстве Х
- Приходится подбирать константу С
    
#### Логистическая регрессия
Логистическая регрессия – популярный алгоритм построения моделей бинарной **классификации** и вероятностного предсказания.
Одной из ее особенностей является возможность оценивания вероятностей классов, тогда как большинство линейных классификаторов могут выдавать только номера классов. 
С помощью данного обработчика можно оценивать вероятность того, что событие наступит для конкретного объекта (больной/здоровый, возврат кредита/дефолт...)

Логистическая регрессия – это разновидность множественной регрессии, предназначенная для классификации записей на основании значений входных полей. 
При этом выходная переменная является категориальной или бинарной (т.е. может принимать только два значения).

В бинарной классификации каждый объект или наблюдение должны быть отнесены к одному из двух классов (например, А и Б). 
Тогда с каждым исходом связано событие: объект принадлежит к классу А и объект принадлежит к классу Б. Результатом будет оценка вероятности соответствующего исхода.

Если в процессе анализа будет установлено, что вероятность принадлежности объекта с заданным набором значений признаков 
(входных переменных) к классу А больше, чем вероятность его принадлежности к классу Б, то он будет классифицирован, как объект класса А.

Например, если рассматривается исход по займу, задается переменная y со значениями 1 и 0, где 1 означает, что соответствующий 
заемщик расплатился по кредиту, а 0, что имел место дефолт.

Несомненным преимуществом логистической регрессии является наличие эффективного инструмента оценки качества моделей - ROC-анализа.

Логарифмическая функция потерь
Оптимизация параметров логистической регрессии
- метод первого порядка - стохастический градиент
- метод второго порядка

Регуляризационная логистическая регрессия
Регуляризация улучшает обобщающую способность логистической регрессии

- L1 - регуляризация имеет эффект отбора признаков (обнуляет веса неинформативных признаков)
- L2 - регуляризация решает проблему мильтиколлинеарности (сокращает веса линейно зависимых признаков)
- ElasticNet -комбинация L1 и L2 для менее агрессивного отбора признаков
























    
    
    