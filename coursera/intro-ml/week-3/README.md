#### НЕДЕЛЯ 3 
#### Метод опорных векторов

`Метод опорных векторов (англ. SVM, support vector machine)` — набор схожих алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. 
Особым свойством метода опорных векторов является непрерывное уменьшение эмпирической ошибки классификации и увеличение зазора, поэтому метод также известен как метод классификатора с максимальным зазором.

Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. 
Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. 

Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.
Данный метод решает задачи классификации и регрессии путем построения нелинейной плоскости, разделяющей решения.

Разделяющая гиперплоскость – это гиперплоскость, которая отделяет группу объектов, имеющих различную классовую принадлежность.

Для построения оптимальной гиперплоскости, SVM прибегает к итерационному алгоритму обучения, использующемуся для минимизации функции ошибок.
В зависимости от вида функции ошибки, SVM модели можно разделить на четыре группы:
1. Классификация SVM типа 1 (также известна как C-SVM классификация)
2. Классификация SVM типа 2 (также известна как Nu-SVM классификация)
3. Регрессия SVM типа 1 (также известна как эпсилон-SVM регрессия)
4. Регрессия SVM типа 2 (также известна как Nu-SVM регрессия)

Ядра для нелинейного обощения SVM
Аппроксимация и регуляризация эмпирического риска
Оптимальная разделяющая гиперплоскость
Переход к линейно неразделимой выборке
Условия Каруша-Куна-Таккера
Ширина разделяющей поверхности

Преимущества метода опорных векторов (SVM)
- Задача выпуклого квадратичного программирования имеет единственное решение
- Выделяется множество опорных векторов
- Имеются эффективгые численные методы для SVM
- Изящное обобщение на нелинейные классификаторы
Недостати метода опорных векторов (SVM)
- Опорными объектами могут становиться объекты выбросы
- Нет отбора признаков в исходном пространстве Х
- Приходится подбирать константу С
    
#### Логистическая регрессия
Логистическая регрессия – популярный алгоритм построения моделей бинарной **классификации** и вероятностного предсказания.
Одной из ее особенностей является возможность оценивания вероятностей классов, тогда как большинство линейных классификаторов могут выдавать только номера классов. 
С помощью данного обработчика можно оценивать вероятность того, что событие наступит для конкретного объекта (больной/здоровый, возврат кредита/дефолт...)

Логистическая регрессия – это разновидность множественной регрессии, предназначенная для классификации записей на основании значений входных полей. 
При этом выходная переменная является категориальной или бинарной (т.е. может принимать только два значения).

В бинарной классификации каждый объект или наблюдение должны быть отнесены к одному из двух классов (например, А и Б). 
Тогда с каждым исходом связано событие: объект принадлежит к классу А и объект принадлежит к классу Б. Результатом будет оценка вероятности соответствующего исхода.

Если в процессе анализа будет установлено, что вероятность принадлежности объекта с заданным набором значений признаков 
(входных переменных) к классу А больше, чем вероятность его принадлежности к классу Б, то он будет классифицирован, как объект класса А.

Например, если рассматривается исход по займу, задается переменная y со значениями 1 и 0, где 1 означает, что соответствующий 
заемщик расплатился по кредиту, а 0, что имел место дефолт.

Несомненным преимуществом логистической регрессии является наличие эффективного инструмента оценки качества моделей - ROC-анализа.

Логарифмическая функция потерь
Оптимизация параметров логистической регрессии
- метод первого порядка - стохастический градиент
- метод второго порядка

Регуляризационная логистическая регрессия
Регуляризация улучшает обобщающую способность логистической регрессии

- L1 - регуляризация имеет эффект отбора признаков (обнуляет веса неинформативных признаков)
- L2 - регуляризация решает проблему мильтиколлинеарности (сокращает веса линейно зависимых признаков)
- ElasticNet -комбинация L1 и L2 для менее агрессивного отбора признаков

#### Метрики качества

В большинстве случаев классификационные модели, например, логистическая регрессия (logistic regression), дают на выходе вероятности в 
интервале от 0 до 1 вместо значений целевой переменой, таких как Да/Нет и т.п. Чтобы оценить точность модели, полученные 
вероятности необходимо преобразовать в значения целевой переменной.

- Доля правильных ответов на выборке - Accuracy
Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма:


- Виды ошибок
Правильное срабатывание - Истинноположительные (ИП). Мошеннические операции, классифицированные, как мошеннические.
Ложное срабатывание - Ложноположительные (ЛП). Добросовестные операции, классифицированные, как мошеннические.
Ложный пропуск - Ложноотрицательные (ЛО). Мошеннические операции, классифицированные, как добросовестные.
Правильный пропуск - Истинноотрицательные (ИО). Добросовестные операции, классифицированные, как добросовестные. 



##### Точность и полнота

Полнота (sensitivity, recall), вычисляется по следующей формуле:
> Полнота = ИП / (ИП + ЛО)

Поскольку формула не учитывает ЛП и ИО, чувствительность может дать нам смещенную оценку, особенно в случае несбалансированных 
классов. В задаче выявления мошенничества чувствительность представляет собой процент правильно классифицированных мошеннических 
операций от общего фактического количества мошеннических операций. Чем выше полнота, тем меньше ложных пропусков

Специфичность (specificity) вычисляется по следующей формуле:
> Специфичность = ИО / (ИО + ЛП)

В задаче выявления мошенничества специфичность представляет собой процент правильно классифицированных добросовестных операций от 
общего фактического количества добросовестных операций.

Точность (precision) вычисляется по следующей формуле:
> Точность = ИП / (ИП + ЛП)

В задаче выявления мошенничества точность представляет собой процент правильно классифицированных мошеннических операций 
от общего количества операций, классифицированных, как мошеннические. Чем выше точность, тем меньше ложных срабатываний


F-мера (Гармоническое среднее, F1 score) представляет собой совместную оценку точности и полноты.
> F-мера = 2 * Точность * Полнота / (Точность + Полнота)

Коэффициент корреляции Мэтьюса
В отличие от остальных метрик, рассмотренных выше, коэффициент корреляции Мэтьюса (ККМ, Matthews сorrelation сoefficient) учитывает 
значения из всех ячеек матрицы сопряженности и вычисляется по следующей формуле:

KMM = (ИП * ИО - ЛП * ЛО) / sqrt((ИП + ЛП) * (ИП + ЛО) * (ИО + ЛП) * (ИО + ЛО)) 

ККМ может принимать значения из интервала от -1 до +1. Модель, получившая оценку +1, является идеальной. Модель, получившая оценку -1, 
является очень слабой. Одним из ключевых свойств ККМ является легкость интерпретации


ROC (Receiver Operating Characteristic - также известна как кривая ошибок) — график, позволяющий оценить качество бинарной классификации, 
    отображает соотношение между долей объектов от общего количества носителей признака, верно классифицированных как несущих признак, 
    (true positive rate, TPR, называемой чувствительностью алгоритма классификации) и долей объектов от общего количества объектов, не несущих 
    признака, ошибочно классифицированных как несущих признак (false positive rate, FPR, величина 1-FPR называется специфичностью алгоритма 
    классификации) при варьировании порога решающего правила.
                                          
Количественную интерпретацию ROC даёт показатель AUC-ROC (Area under ROC curve, площадь под ROC-кривой) — площадь, ограниченная ROC-кривой 
и осью доли ложных положительных классификаций. Чем выше показатель AUC, тем качественнее классификатор, при этом значение 0,5 
демонстрирует непригодность выбранного метода классификации (соответствует случайному гаданию). Значение менее 0,5 говорит, что классификатор 
действует с точностью до наоборот: если положительные назвать отрицательными и наоборот, классификатор будет работать лучше.

AUC-PRC - (Area Under Precision-Recall Curve) - мера качества алгоритма
 - (Area Under Receiver Operating Characteristic) - мера качества алгоритма


#### Многоклассовая классификация

Сведение задачи многоклассовой классификации к бинарной

- One-vs-all
    - Линейное число классификаторов, но каждый обучается на полной выборке
    - Может возникнуть проблема с несбалансированными выборками
- All-vs-all
    - Квадратичное число классификаторов, но каждый обучается на небольшой подвыборке
    





















    
    
    