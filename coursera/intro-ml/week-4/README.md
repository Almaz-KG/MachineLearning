#### НЕДЕЛЯ 4 
#### Линейная регрессия

`Регрессия` – это условное математическое ожидание непрерывной зависимой (выходной) переменной при наблюдаемых значениях независимых (входных) переменных. 
`Линейная регрессия (Linear regression)` —  регрессия основаная на гипотезе, что искомая зависимость – линейная. 
Каждая независимая переменная вносит аддитивный вклад в результирующее значение с некоторым весом, называемом коэффициентом регрессии.

Регрессия называется простой, если входная переменная одна. Однако такая модель является слишком грубым приближением действительности, 
и на практике, как правило, интересны зависимости от нескольких переменных (многомерная регрессия).

Модель многомерной линейной регрессии называется так из-за того, что вычисляет прогноз как линейную комбинацию многих признаков. 
В двумерном случае обученная модель представляет собой гиперплоскость

Построение линейной регрессии заключается в расчете её коэффициентов методом наименьших квадратов. 
Регрессия по методу наименьших квадратов (МНК) часто может стать неустойчивой, то есть сильно зависящей от обучающих данных, 
что обычно является проявлением тенденции к переобучению. Избежать такого переобучения помогает регуляризация - общий метод, заключающийся 
в наложении дополнительных ограничений на искомые параметры, которые могут предотвратить излишнюю сложность модели. 
Смысл процедуры заключается в “стягивании” в ходе настройки вектора коэффициентов  β  таким образом, чтобы они в среднем оказались несколько 
меньше по абсолютной величине, чем это было бы при оптимизации по МНК.

Например, если выходная переменная является категориальной или бинарной, приходится использовать различные модификации регрессии.

Несмотря на свою универсальность, линейная регрессионная модель не всегда пригодна для качественного предсказания зависимой переменной.
Задача многомерной линейной регрессии модет быть решена через сингулярное разложение
Мультиколлинеарность приводит к плохой обусловленности, неустойчивости и переобучению
Методы устранения мультиколлинеарности (гребневая регрессия, метод главных компонент) также связаны с сингулярным разложением

##### Гребневая регрессия
Гребневая регрессия (Ridge-regression) – усовершенствование линейной регрессии с повышенной устойчивостью к ошибкам, налагающая 
ограничения на коэффициенты регрессии для получения куда более приближенного к реальности результата. Вдобавок, этот результат 
гораздо проще интерпретировать. Применяется метод для борьбы с переизбыточностью данных, когда независимые переменные коррелируют 
друг с другом (мультиколлинеарность). Это модель многомерной линейной регрессии с L2 регуляризатором. 

##### Метод LASSO
Лассо-регрессия сходна с гребневой, за исключением того, что коэффициенты регрессии могут равняться нулю (часть признаков при этом исключается из модели).

Сравнение 
- Гребневая регрессия  удобно вводится и интерпретируется через сингулярное разложение
- Гребневая регрессия сокращает веса признаков
- Lasso обнуляет веса признаков
- Оба метода имеют параметр регуляризации (селективности), позволяющий определять число признаков (сложность модели) по внешним критериям (по кросс-валидации)


#### Понижение размерности и метод главных компонент
Метод главных компонент (Principal component analysis - PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации, изобретён Карлом Пирсоном в 1901 году. 
Метод главных компонент (Principal component analysis - PCA) — один из основных методов обучения без учителя, который позволяет сформировать 
новые признаки, являющиеся линейными комбинациями старых. При этом новые признаки строятся так, чтобы сохранить как можно 
больше дисперции в данных. 

Основным параметром метода главных компонент является количество новых признаков. Как и в большинстве методов машинного обучения, 
нет четких рекомендаций по поводу выбора значения этих параметров. Один из подходов - выбрать минимальное число компонент, при котором
все еще объясняется некоторая закономерность.

Задача анализа главных компонент имеет, как минимум, четыре базовых версии:
- аппроксимировать данные линейными многообразиями меньшей размерности;
- найти подпространства меньшей размерности, в ортогональной проекции на которые разброс данных (то есть среднеквадратичное отклонение от среднего значения) максимален;
- найти подпространства меньшей размерности, в ортогональной проекции на которые среднеквадратичное расстояние между точками максимально;
- для данной многомерной случайной величины построить такое ортогональное преобразование координат, в результате которого корреляции между отдельными координатами обратятся в нуль.

### TODO: Дополнительные замечания и дополнения сюда


Основная теорема метода главных компонент
Эффективная размерность выборки 

Метод главных компонент позволяет приближать матрицу ее низкоранговым разложением
Для этого достаточно из SVD-разложения первые m сингулярных чисел из векторов матрицы
Этот прием широко используется в анализе данных - в задачах регрессии, классификации, сжатия данных, обработки извображений


##### Дополнительные материалы
[Как работает метод главных компонент (PCA) на простом примере](https://habr.com/post/304214/)
[Метод главных компонент](http://math-info.hse.ru/f/2015-16/ling-mag-quant/lecture-pca.html)
[Анализ главных компонент (Principal Component Analysis, PCA)](https://github.com/lanit-tercom-school/analyzeme/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85-%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82-(Principal-Component-Analysis,-PCA))


