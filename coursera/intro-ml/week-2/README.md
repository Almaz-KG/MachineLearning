#### НЕДЕЛЯ 2 
#### Метод k ближайщих соседей

Метод k-ближайших соседей (англ. k-nearest neighbors algorithm, k-NN) — метрический алгоритм для автоматической классификации объектов или регрессии, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.
- В случае использования метода для классификации объект присваивается тому классу, который является наиболее распространённым среди k соседей данного элемента, классы которых уже известны.
- В случае использования метода для регрессии, объекту присваивается среднее значение по k ближайшим к нему объектам, значения которых уже известны.

Для повышения надёжности классификации объект относится к тому классу, которому принадлежит большинство из его соседей — k ближайших к нему объектов обучающей выборки. В задачах с двумя классами число соседей берут нечётным, чтобы не возникало ситуаций неоднозначности, когда одинаковое число соседей принадлежат разным классам. В задачах с числом классов 3 и более нечётность уже не помогает, и ситуации неоднозначности всё равно могут возникать.

##### Гипотеза компактности
Все перечисленные методы неявно опираются на одно важное предположение, называемое гипотезой компактности: если мера сходства объектов введена достаточно удачно, то схожие объекты гораздо чаще лежат в одном классе, чем в разных. В этом случае граница между классами имеет достаточно простую форму, а классы образуют компактно локализованные области в пространстве объектов.

Гипотеза непрерывности
> Близким объектам соответствуют близкие ответы 

Гипотеза компактности
> Близкие объекты, как правило, лежат в одном классе

### Линейные методы классификации
Линейный классификатор — алгоритм классификации, основанный на построении линейной разделяющей поверхности. Класс задач, которые можно решать с помощью линейных классификаторов, обладают, соответственно, свойством линейной сепарабельности

Два множества точек в двумерном пространстве называются `линейно сепарабельными` (линейно разделимыми), если они могут быть полностью отделены единственной прямой. Для n-мерного пространства два набора точек линейно разделимы, если они могут быть отделены (n−1)-мерной гиперплоскостью.

Операцию линейной классификации для двух классов можно себе представить как отображение объектов в многомерном пространстве на гиперплоскость, в которой те объекты, которые попали по одну сторону разделяющей линии, относятся к первому классу ("да"), а объекты по другую сторону - ко второму классу ("нет")).
Линейный классификатор используется когда важно проводить быстрые вычисления с большой скоростью. Он неплохо работает, когда входной вектор {\displaystyle {\vec {x}}} {\vec  x} разрежен. Линейные классификаторы могут хорошо срабатывать в многомерном пространстве, например, для классификации документов по матрице встречаемости слов. В подобных случаях считается, что объекты хорошо регуляризируемы.

`Отступом` называется эвристика, оценивающая то, насколько объект "погружён" в свой класс, насколько эталонным представителем он является. Чем меньше значение отступа, тем ближе объект находится к границе класса, соответственно тем выше вероятность ошибочного прогноза.
Отрицательный отступ означает, что объект классифицирован неправильно, положительный — правильно. Модуль отступа показывает, насколько классификатор был "уверен" в ответе.

Выбросы — объекты, классифицируемые неправильно, с большим отрицательным значением отступа.

Пограничные объекты — объекты с малым по модулю отступом.

Типичные объекты — объекты, которые правильно классифицированы, с большим значением оступа.

Эталоны — объекты с наибольшим значением отступа.

Эмпирический риск (Empirical Risk) — это средняя величина ошибки алгоритма на обучающей выборке.

#### Градиентные методы численной минимизации и алгоритм SG
Градиентный спуск
Градиентный шаг
Алгоритм SG
Алгоритм SAG

#### Достоинства и недостатки метода стохастического градиента
Достоинства 
- Легко реализуется
- Применим к любым моделям и функциям потерь
- Допускает онлайновое (потоковое) обучение
- На сверхбольших больших выборках позволяет получать неплохие решения, даже не обработав всю обучающую выборку

Недостатки
- Возможно застревание в локальных экстремумах
- Возможна расходимость или медленная скорость спуска
- Возможно переобучение
- Подбор комплекса эвристик является искусством

#### Проблема переобучения
Регуляризация решает проблему мультиколлиенеарности и снижает риск переобучения


`Метод k ближайщих соседей`, `KNN`, `Гипотеза компактности`, `Гипотеза непрерывности`


