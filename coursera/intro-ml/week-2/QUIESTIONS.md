Метрические методы
##### 1. В чем заключается процесс обучения алгоритма k ближайщих соседей
- [ ] Настройка весов при объектах обучающей выборки
- [x] Запоминании всех объектов обучающей выборки 
- [ ] Посчет попарных расстояний между объектами обучающей выборки

##### 2. Как устроена формула ядерного сглаживания Надарая-Ватсона
- [ ] Она усредняется ответы всех объектов обучающей выборки с равными весами
- [ ] Она выдает ответ объекта обучающей выборки, ближайшего к данному
- [ ] Она выдает средний ответ k объектов обучающей выборки, ближайщих к данному
- [x] Она усредняет ответы всех объектов обучающей выборки с весами, обратно пропорциональными расстоянию до них


##### 3. Выберите верные утверждения про отступ (margin)
- [x] Если отступ близок к нулю, надежность классификации этого объекта не высока
- [x] Если отступ на данном объекте больше нуля, то алгоритм дает верный ответ 
- [ ] Если отступ на данном объекте меньше нуля, то алгоритм дает верный ответ


##### 4. Выберите верные утверждения про настройку линейных классификаторов
- [x] Хороший линейный классификатор должен минимизировать эмпирический риск - то есть число ошибок на обучающей выборке
- [ ] Минимизация эмпирического риска приводит к переобучению, поэтому он заменяется на другой функционал
- [x] Минимизация эмпирического риска - это очень сложная оптимизационная задача, поэтому он заменяется на другой функционал
- [ ] Решение задачи минимизации эмпирического риска можно записать в явном виде